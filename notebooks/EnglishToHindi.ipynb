{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2w3QYsb8VPFL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hindi=pd.read_csv('/content/datasethindi.csv')"
      ],
      "metadata": {
        "id": "AF4saGB0VgpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "xtrain,xtest,ytrain,ytest=train_test_split(hindi[\"English\"],hindi[\"Hindi\"],test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "8e1ib5F5Vivi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, TimeDistributed\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "X60PenKlVkfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain=[str(x) for x in xtrain]   #to ensure they are strings\n",
        "ytrain=[str(y) for y in ytrain]\n",
        "xtest=[str(x) for x in xtest]\n",
        "ytest=[str(y) for y in ytest]"
      ],
      "metadata": {
        "id": "YBetBVClVmbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en = Tokenizer(num_words=20000, oov_token=None)   #keeping only top 20000 freq words, no specil token for oov words\n",
        "hi = Tokenizer(num_words=20000, oov_token=None)\n",
        "en.fit_on_texts(xtrain+xtest)\n",
        "hi.fit_on_texts(ytrain+ytest)"
      ],
      "metadata": {
        "id": "u5W25cRvVpWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(\"en_tokenizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(en, f)\n",
        "with open(\"hi_tokenizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(hi, f)"
      ],
      "metadata": {
        "id": "_7MQaP0zaIff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(en.word_index),len(hi.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ju-7qVqVq8H",
        "outputId": "f67be264-64f4-4203-b0cd-a36845eef5e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28706, 35696)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en.document_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHx7DBLBVtfT",
        "outputId": "c57a4a21-b9e9-454e-963a-5435373c24c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "73243"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "invocab=len(en.word_index)+1\n",
        "outvocab=len(hi.word_index)+1"
      ],
      "metadata": {
        "id": "QMODqEMwVuWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain=en.texts_to_sequences(xtrain)   #words replaced by their corresp index from tokenizer\n",
        "xtest=en.texts_to_sequences(xtest)\n",
        "ytrain=hi.texts_to_sequences(ytrain)\n",
        "ytest=hi.texts_to_sequences(ytest)"
      ],
      "metadata": {
        "id": "hwCZ4yxRVv4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mxlen=150    #max seq length\n",
        "xtrain=pad_sequences(xtrain,maxlen=mxlen,padding='post',truncating='post')   #padding at end of seqs\n",
        "xtest=pad_sequences(xtest,maxlen=mxlen,padding='post',truncating='post')\n",
        "ytrain= pad_sequences(ytrain,maxlen=mxlen,padding='post',truncating='post')\n",
        "ytest= pad_sequences(ytest,maxlen=mxlen,padding='post',truncating='post')"
      ],
      "metadata": {
        "id": "yO9HkpJZVxrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ytrain=ytrain.reshape(*ytrain.shape,1)     ##reshaping and adding extra dimension, req for seq2seq models with sparse categorical cross entropy so that preds (batch,seqlen,vocabsize) align with (batch,seqlen,1)\n",
        "ytest=ytest.reshape(*ytest.shape,1)"
      ],
      "metadata": {
        "id": "xZ2dRvBxVzwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class transblock(tf.keras.layers.Layer):\n",
        "    def __init__(self,embdim,heads,ffdim,rate=0.1,**kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embdim=embdim     #embedding dimension\n",
        "        self.heads=heads     #attention heads\n",
        "        self.ffdim=ffdim       #dimension for FF layer\n",
        "        self.rate=rate            #dropout rate\n",
        "        self.att=tf.keras.layers.MultiHeadAttention(num_heads=heads,key_dim=embdim)    #multi head attention layer (looks at all tokens from seq and builds contextual embeddings that remmber context)\n",
        "        self.ff=tf.keras.Sequential([            #FF network\n",
        "            tf.keras.layers.Dense(ffdim,activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(embdim),\n",
        "        ])\n",
        "        self.ln1=tf.keras.layers.LayerNormalization(epsilon=1e-6)    #layer norm to prevent vanishing gradient problem\n",
        "        self.ln2=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dp1=tf.keras.layers.Dropout(rate)      #Dropout to reduce overfitting\n",
        "        self.dp2=tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def get_config(self):        #serialisable configs(to save and load models later)\n",
        "        cfg=super().get_config().copy()\n",
        "        cfg.update({\n",
        "            'embdim':self.embdim,\n",
        "            'heads':self.heads,\n",
        "            'ffdim':self.ffdim,\n",
        "            'rate':self.rate,\n",
        "        })\n",
        "        return cfg\n",
        "\n",
        "    def call(self,x,training=None):\n",
        "        att=self.att(x,x)\n",
        "        att=self.dp1(att,training=training)\n",
        "        out1=self.ln1(x+att)\n",
        "        ffout=self.ff(out1)\n",
        "        ffout=self.dp2(ffout,training=training)\n",
        "        return self.ln2(out1+ffout)\n"
      ],
      "metadata": {
        "id": "rkYK8t6BV3Qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class tokposemb(tf.keras.layers.Layer):    #token+position embedding layer\n",
        "   def __init__(self,maxlen,vocab,embdim,**kw):\n",
        "        super().__init__(**kw)\n",
        "        self.maxlen=maxlen\n",
        "        self.vocab=vocab\n",
        "        self.embdim=embdim\n",
        "        self.tokemb=tf.keras.layers.Embedding(input_dim=vocab,output_dim=embdim)    #token embedding maps word indicesto dense vectors\n",
        "        self.posemb=tf.keras.layers.Embedding(input_dim=maxlen,output_dim=embdim)     #pos emb -> same dimension as token emb\n",
        "\n",
        "   def get_config(self):\n",
        "        cfg=super().get_config().copy()\n",
        "        cfg.update({\n",
        "            'maxlen':self.maxlen,\n",
        "            'vocab':self.vocab,\n",
        "            'embdim':self.embdim,\n",
        "        })\n",
        "        return cfg\n",
        "\n",
        "   def call(self,x):\n",
        "        ln=tf.shape(x)[-1]    #dynamic seq len of input\n",
        "        pos=tf.range(start=0,limit=ln,delta=1)   #creating poisition emb(easier app as dataset is small and emb stays within linit)\n",
        "        pos=self.posemb(pos)\n",
        "        x=self.tokemb(x)\n",
        "        return x+pos    #adding embeddings(token+pos) same as official transformers\n"
      ],
      "metadata": {
        "id": "ILIQeu5CV7a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mini transformer seq2seq model\n",
        "heads=2        # fewer att heads for light attn\n",
        "ffdim=32\n",
        "embdim=128     # smaller emb to save memory\n",
        "opt=Adam(learning_rate=0.003)   #optimiser\n",
        "\n",
        "inp=tf.keras.layers.Input(shape=(mxlen,))\n",
        "emb=tokposemb(mxlen, invocab, embdim)(inp)    #token+pos emb layer\n",
        "x=transblock(embdim, heads, ffdim)(emb)         #encoder block (self att+FF layer)\n",
        "x=TimeDistributed(Dense(128, activation=\"relu\"))(x)  #applying dense to each timestamp separately\n",
        "out=TimeDistributed(Dense(outvocab, activation=\"softmax\"))(x)        #softmax for output vocab\n",
        "\n",
        "model=tf.keras.Model(inputs=inp,outputs=out)\n",
        "model.compile(loss=sparse_categorical_crossentropy,optimizer=opt,metrics=['accuracy'])    #loss is scce coz targets are int ids not one hot encoded ones\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "psmVlt8gV9YL",
        "outputId": "0de8b4dd-8592-497d-e2f5-2fda543f4e6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ tokposemb_2 (\u001b[38;5;33mtokposemb\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m3,693,696\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transblock_2 (\u001b[38;5;33mtransblock\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m140,832\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_4              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m16,512\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_5              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m35697\u001b[0m)     │     \u001b[38;5;34m4,604,913\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ tokposemb_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">tokposemb</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,693,696</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transblock_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">transblock</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">140,832</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_4              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_5              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35697</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,604,913</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,455,953\u001b[0m (32.26 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,455,953</span> (32.26 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,455,953\u001b[0m (32.26 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,455,953</span> (32.26 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(\n",
        "    xtrain,\n",
        "    ytrain,\n",
        "    validation_data=(xtest,ytest),\n",
        "    verbose=1,\n",
        "    batch_size=16,\n",
        "    epochs=5,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_ijNeyRV_Ka",
        "outputId": "5f4668e1-aff4-4468-e5a6-62fe6776a077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m3663/3663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m632s\u001b[0m 143ms/step - accuracy: 0.9573 - loss: 0.4828 - val_accuracy: 0.9598 - val_loss: 0.3472\n",
            "Epoch 2/5\n",
            "\u001b[1m3663/3663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m414s\u001b[0m 112ms/step - accuracy: 0.9600 - loss: 0.3396 - val_accuracy: 0.9602 - val_loss: 0.3418\n",
            "Epoch 3/5\n",
            "\u001b[1m3663/3663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m461s\u001b[0m 118ms/step - accuracy: 0.9606 - loss: 0.3225 - val_accuracy: 0.9606 - val_loss: 0.3296\n",
            "Epoch 4/5\n",
            "\u001b[1m3663/3663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m431s\u001b[0m 118ms/step - accuracy: 0.9616 - loss: 0.2949 - val_accuracy: 0.9611 - val_loss: 0.3207\n",
            "Epoch 5/5\n",
            "\u001b[1m3663/3663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m423s\u001b[0m 112ms/step - accuracy: 0.9625 - loss: 0.2676 - val_accuracy: 0.9615 - val_loss: 0.3132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samps = [\"i love you\",\"she is beautiful\"]\n",
        "for s in samps:\n",
        "    seq=en.texts_to_sequences([s])      #eng sentences to seq of token ids\n",
        "    padseq=pad_sequences(seq, maxlen=mxlen,padding='post',truncating='post')    #padding seq to our fixed len\n",
        "    pred=model.predict(padseq)[0].argmax(1)           #predicting hindi token probabilities and taking token with max probability\n",
        "    out=hi.sequences_to_texts([pred])[0]\n",
        "    print(\"EN:\",s)\n",
        "    print(\"HI:\",out,\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPMZwSA5efuj",
        "outputId": "c5d744b7-3311-4b5c-cba5-109372dfd3ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 14s/step\n",
            "EN: i love you\n",
            "HI: मुझे प्यार प्यार \n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "EN: she is beautiful\n",
            "HI: ये सुंदर \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"besteng2hindi.keras\")"
      ],
      "metadata": {
        "id": "sv-nM7irelul"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}