{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2w3QYsb8VPFL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, TimeDistributed,Input,LayerNormalization,Dropout\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "Fnx-WWnbmqQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv('/content/datasethindi.csv')"
      ],
      "metadata": {
        "id": "AF4saGB0VgpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng = data['English'].astype(str).tolist()\n",
        "hin= data['Hindi'].astype(str).tolist()"
      ],
      "metadata": {
        "id": "rLtZiXFrIs5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hintok = [\"<start> \" + s.strip() + \" <end>\" for s in hin]"
      ],
      "metadata": {
        "id": "dRZP04ebI1Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numwords = 20000\n",
        "en = Tokenizer(num_words=numwords, oov_token=\"<unk>\", filters='', lower=True)     #keeping only 20000 words, replacing out of vocab token with <unk>\n",
        "hi = Tokenizer(num_words=numwords, oov_token=\"<unk>\", filters='', lower=True)\n",
        "en.fit_on_texts(eng)        #learns unique words and assigns ids\n",
        "hi.fit_on_texts(hintok)"
      ],
      "metadata": {
        "id": "-77icCCjI5WI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(\"en.pkl\", \"wb\") as f:\n",
        "    pickle.dump(en, f)\n",
        "with open(\"hi.pkl\", \"wb\") as f:\n",
        "    pickle.dump(hi, f)"
      ],
      "metadata": {
        "id": "T-sp2Tur5RbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "invocab = min(numwords, len(en.word_index)+1)     #min(20000,tot eng words+1->for padding)\n",
        "outvocab = min(numwords, len(hi.word_index)+1)\n",
        "startid = hi.word_index.get(\"<start>\")    #adding start and end tokens\n",
        "endid = hi.word_index.get(\"<end>\")\n",
        "print(\"invocab\", invocab, \"outvocab\", outvocab, \"startid\", startid, \"endid\", endid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdC5GdV3JCjT",
        "outputId": "08ad20ff-00da-4e67-8141-7f166192f85a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "invocab 20000 outvocab 20000 startid 2 endid 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "engseqs = en.texts_to_sequences(eng)       #eng sentences to word ids\n",
        "hinseqs = hi.texts_to_sequences(hintok)\n",
        "mxlen = 150       #longer ones will be turncated\n",
        "engpad = pad_sequences(engseqs, maxlen=mxlen, padding='post', truncating='post')  #padding\n",
        "hinpad = pad_sequences(hinseqs, maxlen=mxlen, padding='post', truncating='post')\n",
        "xtrain, xtest, ytrainfull, ytestfull = train_test_split(engpad, hinpad, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "q1BAfNVlJG1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decintrain = np.zeros_like(ytrainfull)        #decoder input for training\n",
        "decintrain[:,0] = startid          #first is strt token\n",
        "decintrain[:,1:] = ytrainfull[:,:-1]        #teacher forcing\n",
        "decintest = np.zeros_like(ytestfull)\n",
        "decintest[:,0] = startid\n",
        "decintest[:,1:] = ytestfull[:,:-1]        #shifting by 1, for calculating test loss/metrics and not for real preds\n",
        "ytrain = ytrainfull[..., np.newaxis]             #extra dim as we are using scce\n",
        "ytest = ytestfull[..., np.newaxis]"
      ],
      "metadata": {
        "id": "1os87azcJN2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class transblock(tf.keras.layers.Layer):             #encoder block\n",
        "    def __init__(self, embdim, heads, ffdim, rate=0.1, **kw):\n",
        "        super().__init__(**kw)\n",
        "        keydim = max(1, embdim // heads)           #dim per att head\n",
        "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=heads, key_dim=keydim)      #multi head self attention\n",
        "        self.ff = tf.keras.Sequential([Dense(ffdim, activation=\"relu\"), Dense(embdim)])        #FFNN\n",
        "        self.ln1 = LayerNormalization(epsilon=1e-6)            #Layer Normalisation to stabilise training\n",
        "        self.ln2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dp1 = Dropout(rate)      #for regularisation\n",
        "        self.dp2 = Dropout(rate)\n",
        "    def call(self, x, training=None):\n",
        "        a = self.att(x, x)          #query*key+val\n",
        "        a = self.dp1(a, training=training)\n",
        "        x = self.ln1(x+a)         #residual connection+layer norm\n",
        "        f = self.ff(x)\n",
        "        f = self.dp2(f, training=training)\n",
        "        return self.ln2(x+f)            #again r+l\n"
      ],
      "metadata": {
        "id": "og5d0W39JRSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class decoderblock(tf.keras.layers.Layer):          #decoder\n",
        "    def __init__(self, embdim, heads, ffdim, rate=0.1, **kw):\n",
        "        super().__init__(**kw)\n",
        "        keydim = max(1, embdim // heads)   #dim per head\n",
        "        self.selfatt = tf.keras.layers.MultiHeadAttention(num_heads=heads, key_dim=keydim)         #masked self att\n",
        "        self.crossatt = tf.keras.layers.MultiHeadAttention(num_heads=heads, key_dim=keydim)           #cross att leyer\n",
        "        self.ff = tf.keras.Sequential([Dense(ffdim, activation=\"relu\"), Dense(embdim)])\n",
        "        self.ln1, self.ln2, self.ln3 = LayerNormalization(epsilon=1e-6), LayerNormalization(epsilon=1e-6), LayerNormalization(epsilon=1e-6)   #3 layer norms for (after self att,cross att,FFNN)\n",
        "        self.dp1, self.dp2, self.dp3 = Dropout(rate), Dropout(rate), Dropout(rate) #3 dropout layers\n",
        "    def call(self, x, encout, training=None):\n",
        "        ln = tf.shape(x)[1]\n",
        "        mask = tf.linalg.band_part(tf.ones((ln, ln)), -1, 0)      #upper triangle as 0s so token cannot see future ones\n",
        "        s = self.selfatt(x, x, attention_mask=mask)      #masked self att**\n",
        "        s = self.dp1(s, training=training)\n",
        "        x = self.ln1(x+s)\n",
        "        c = self.crossatt(x, encout)       #query=decoder ip and key=val=encoder inp\n",
        "        c = self.dp2(c, training=training)\n",
        "        x = self.ln2(x+c)       #residual+norm\n",
        "        f = self.ff(x)\n",
        "        f = self.dp3(f, training=training)\n",
        "        return self.ln3(x+f)            #r+l again"
      ],
      "metadata": {
        "id": "ZU4uNinuJUq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class tokposemb(tf.keras.layers.Layer):           #simple token+pos emb layer\n",
        "    def __init__(self, maxlen, vocab, embdim, **kw):\n",
        "        super().__init__(**kw)\n",
        "        self.tokemb = Embedding(vocab, embdim)\n",
        "        self.posemb = Embedding(maxlen, embdim)\n",
        "    def call(self, x):\n",
        "        ln = tf.shape(x)[-1]               #dynamic seq len\n",
        "        pos = tf.range(start=0, limit=ln, delta=1)           #pos indices\n",
        "        return self.tokemb(x) + self.posemb(pos)         #adding them"
      ],
      "metadata": {
        "id": "dv0Ayj7WJXlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numlayers = 2\n",
        "heads = 4              #multi attention heads\n",
        "embdim = 128\n",
        "ffdim = 256\n",
        "opt = Adam(learning_rate=1e-4, clipnorm=1.0)            #adam with gradient clipping to prevent exploding grad\n",
        "\n",
        "encinp = Input(shape=(mxlen,))\n",
        "decinp = Input(shape=(mxlen,))        #hin sentences ids shifted with start\n",
        "enc = tokposemb(mxlen, invocab, embdim)(encinp)         #token +pos\n",
        "dec = tokposemb(mxlen, outvocab, embdim)(decinp)\n",
        "for _ in range(numlayers):          #pass them thru 2 blocks\n",
        "    enc = transblock(embdim, heads, ffdim)(enc)\n",
        "for _ in range(numlayers):\n",
        "    dec = decoderblock(embdim, heads, ffdim)(dec, enc)          #eacch block with aelf att+cross att with enc op\n",
        "out = TimeDistributed(Dense(outvocab, activation=\"softmax\"))(dec)           #for each t, taking op prob dist over op vocab\n",
        "\n",
        "model = tf.keras.Model([encinp, decinp], out)\n",
        "model.compile(loss=sparse_categorical_crossentropy, optimizer=opt, metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "xnnF0gBgJcmj",
        "outputId": "d298a4fe-cf01-4e3f-ed6b-e68fb23ea89b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ tokposemb_4         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │  \u001b[38;5;34m2,579,200\u001b[0m │ input_layer_4[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mtokposemb\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_5       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transblock_2        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │    \u001b[38;5;34m132,480\u001b[0m │ tokposemb_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mtransblock\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ tokposemb_5         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │  \u001b[38;5;34m2,579,200\u001b[0m │ input_layer_5[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mtokposemb\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transblock_3        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │    \u001b[38;5;34m132,480\u001b[0m │ transblock_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mtransblock\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoderblock        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │    \u001b[38;5;34m198,784\u001b[0m │ tokposemb_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mdecoderblock\u001b[0m)      │                   │            │ transblock_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoderblock_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │    \u001b[38;5;34m198,784\u001b[0m │ decoderblock[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mdecoderblock\u001b[0m)      │                   │            │ transblock_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ time_distributed    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m,       │  \u001b[38;5;34m2,580,000\u001b[0m │ decoderblock_1[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ \u001b[38;5;34m20000\u001b[0m)            │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ tokposemb_4         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,579,200</span> │ input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">tokposemb</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_5       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transblock_2        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">132,480</span> │ tokposemb_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">transblock</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ tokposemb_5         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,579,200</span> │ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">tokposemb</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transblock_3        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">132,480</span> │ transblock_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">transblock</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoderblock        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">198,784</span> │ tokposemb_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">decoderblock</span>)      │                   │            │ transblock_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoderblock_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">198,784</span> │ decoderblock[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">decoderblock</span>)      │                   │            │ transblock_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ time_distributed    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>,       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,580,000</span> │ decoderblock_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">20000</span>)            │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,400,928\u001b[0m (32.05 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,400,928</span> (32.05 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,400,928\u001b[0m (32.05 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,400,928</span> (32.05 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit([xtrain, decintrain], ytrain,\n",
        "                    validation_data=([xtest, decintest], ytest),\n",
        "                    batch_size=32, epochs=10, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5QKS190JeRz",
        "outputId": "94516982-0262-41bc-d8b8-c276e88a8e1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1832/1832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 122ms/step - accuracy: 0.9654 - loss: 0.2314 - val_accuracy: 0.9630 - val_loss: 0.2693\n",
            "Epoch 2/10\n",
            "\u001b[1m1832/1832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 118ms/step - accuracy: 0.9669 - loss: 0.2152 - val_accuracy: 0.9637 - val_loss: 0.2637\n",
            "Epoch 3/10\n",
            "\u001b[1m1832/1832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 122ms/step - accuracy: 0.9684 - loss: 0.1998 - val_accuracy: 0.9641 - val_loss: 0.2607\n",
            "Epoch 4/10\n",
            "\u001b[1m1832/1832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 119ms/step - accuracy: 0.9696 - loss: 0.1873 - val_accuracy: 0.9646 - val_loss: 0.2578\n",
            "Epoch 5/10\n",
            "\u001b[1m1832/1832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 119ms/step - accuracy: 0.9711 - loss: 0.1730 - val_accuracy: 0.9649 - val_loss: 0.2568\n",
            "Epoch 6/10\n",
            "\u001b[1m1832/1832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 122ms/step - accuracy: 0.9727 - loss: 0.1601 - val_accuracy: 0.9651 - val_loss: 0.2568\n",
            "Epoch 7/10\n",
            "\u001b[1m1832/1832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 121ms/step - accuracy: 0.9737 - loss: 0.1505 - val_accuracy: 0.9655 - val_loss: 0.2576\n",
            "Epoch 8/10\n",
            "\u001b[1m1832/1832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 121ms/step - accuracy: 0.9751 - loss: 0.1399 - val_accuracy: 0.9656 - val_loss: 0.2590\n",
            "Epoch 9/10\n",
            "\u001b[1m1832/1832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 118ms/step - accuracy: 0.9763 - loss: 0.1306 - val_accuracy: 0.9658 - val_loss: 0.2614\n",
            "Epoch 10/10\n",
            "\u001b[1m1832/1832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 121ms/step - accuracy: 0.9775 - loss: 0.1219 - val_accuracy: 0.9659 - val_loss: 0.2640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(model, src, maxlen, startid, endid):\n",
        "    dec = np.zeros((1, maxlen))            #decoder ip arr, all 0s at start\n",
        "    dec[0,0] = startid #start token\n",
        "    for t in range(1, maxlen):\n",
        "        p = model.predict([src, dec], verbose=0)       #encoder ip+curr decoder ip,  shape=(1,mxlen,voacbsize)\n",
        "        nid = np.argmax(p[0,t-1])           #picking highest prob token at each step\n",
        "        dec[0,t] = nid      #adding it to decoder ip\n",
        "        if nid == endid: break      #end token is last\n",
        "    return dec[0]       #return pred seq\n",
        "\n",
        "def ids_to_text(ids, tok):\n",
        "    out = []\n",
        "    for i in ids:\n",
        "        if i in (0, startid, endid): continue\n",
        "        w = tok.index_word.get(int(i), \"\")      #id->word\n",
        "        if w: out.append(w)\n",
        "    return \" \".join(out)\n",
        "\n",
        "samps = [\"leave door open\",\"king organised meet\"]\n",
        "for s in samps:\n",
        "    seq = en.texts_to_sequences([s])    #word->token ids\n",
        "    padseq = pad_sequences(seq, maxlen=mxlen, padding='post')\n",
        "    pred = greedy_decode(model, padseq, mxlen, startid, endid)    #greedy decoding, one toekn at a time\n",
        "    print(\"EN:\", s)\n",
        "    print(\"HI:\", ids_to_text(pred, hi))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsDiNIkzJiUc",
        "outputId": "740b80f4-d56a-402a-bf2d-bbd6ddeebe3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EN: leave door open\n",
            "HI: दरवाज़ा खोलो।\n",
            "EN: king organised meet\n",
            "HI: राजा रूप रामचरितमानस तुलसी घर ले जाता\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"besteng2hindi2.keras\")"
      ],
      "metadata": {
        "id": "QNEN-xgOJlwN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}